{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Assignment 2: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9xqCFJBv_WUt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\josho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8sr9x17KhU"
   },
   "source": [
    "## Task 1: Named Entity Annotation (10 Marks)\n",
    "\n",
    "Using the IOB tagging scheme annotate all of the named entities (PERson, LOCation, ORGanisation, TIME) in the following sentence:\n",
    "\n",
    "*Wayne Rooney is a professional footballer from England who last played for Major League Soccer club D.C. United and will join Derby County in January 2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htlSW1ad81D-"
   },
   "source": [
    "Edit this cell and write your annotation below the line. (Note that you don't have to write code for this task, you have to annotate it manually)\n",
    "\n",
    "---\n",
    "(PER Wayne Rooney) is a (POS professional footballer) from (LOC England) who played for Major League Soccer club (ORG D.C. United) and will join (ORG Derby Vounty) in (TIME January 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNmDWxj-V-J"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### For subsequent tasks in this assignment, you will work with the documents in `football_players.txt` to perform various information extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V9YE4n6u7olU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 28 24172   28  6848    0     0   5243      0  0:00:04  0:00:01  0:00:03  5251\n",
      "100 24172  100 24172    0     0  17734      0  0:00:01  0:00:01 --:--:-- 17747\n"
     ]
    }
   ],
   "source": [
    "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
    "!curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpSaij2b73Vj"
   },
   "source": [
    " Read all the documents from `football_players.txt` into a list called `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qteh89cs7q4x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zlatan Ibrahimović (Swedish pronunciation: [ˈslaːtan ɪbraˈhiːmɔvɪtɕ] , Bosnian: [zlǎtan ibraxǐːmoʋitɕ] ; born 3 October 1981) is a Swedish professional footballer who plays as a striker for Manchester United. He was also a member of the Sweden national team, making his senior international debut in 2001 and serving as captain from 2010 until he retired from international football in 2016. Ibrahimović began his career at Malmö FF in the late 1990s before being signed by Ajax, where he made a name for himself. He signed for Juventus and excelled in Serie A in a strike partnership with David Trezeguet. In 2006, he signed for rival side Inter Milan and was named to the UEFA Team of the Year in both 2007 and 2009. In addition, Ibrahimović would finish as the league's top scorer in 2008–09 and win three straight Scudetti. In the summer of 2009, he transferred to Barcelona, before moving back to Serie A football the following season, joining A.C. Milan in a deal that made him one of the highest-paid players in the world. He won another Scudetto with Milan in the 2010–2011 season. He joined Paris Saint-Germain in July 2012. During his four-season stay at PSG, Ibrahimović won four consecutive Ligue 1 titles, three Coupes de la Ligue, two Coupes de France and was the top scorer in Ligue 1 for three seasons. In October 2015, he became PSG's all time leading goalscorer. He finished his PSG career with 156 goals in 180 competitive matches. Ibrahimović is one of ten players to have made 100 or more appearances for the Swedish national team. He is the country's all-time leading goalscorer with 62 goals. He represented Sweden at the 2002 and 2006 FIFA World Cups, as well as the 2004, 2008, 2012, and 2016 UEFA European Championships. He has been awarded Guldbollen (the Golden Ball), given to the Swedish player of the year, a record ten times, including nine consecutive times from 2007 and 2015. With his playing style and acrobatic finishing compared to Dutch retired striker Marco van Basten, Ibrahimović is widely regarded as one of the best strikers in the game and one of the best footballers of his generation. His spectacular bicycle kick for Sweden against England won the 2013 FIFA Puskás Award for Goal of the Year. Off the field, he is known for his brash persona and outspoken comments, in addition to referring to himself in the third person. In December 2013, Ibrahimović was ranked by The Guardian as the third-best player in the world, behind only Lionel Messi and Cristiano Ronaldo. In December 2014, the Swedish newspaper Dagens Nyheter named him the second-greatest Swedish sportsperson of all time, after tennis player Björn Borg.\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "# for some strange reason specifying the encoding variable was the only way I could get open() to work]\n",
    "docs = open(\"football_players.txt\", 'r', encoding = 'utf-8').readlines()\n",
    "docs[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "## Task 2 (10 Marks)\n",
    "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7Fyfq7DYxW"
   },
   "source": [
    "Hint: For this task you need to perform three steps:\n",
    "1. Sentence Segmentation\n",
    "1. Word Tokenization\n",
    "1. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 approach\n",
    "***\n",
    "I began by using ntlk's _sent_tokenize()_ function from the excercise sheets to split the document into sentences.  \n",
    "  \n",
    "  I then tokenised each of these sentences using ntlk's _tokenize()_ function in a list comprehension.  \n",
    "    \n",
    "  Finally I used nltk's _pos_tag_ function to tag each of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U3MCJIcR_WU2"
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "  \n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    tokens = [nltk.word_tokenize(sentence.strip()) for sentence in sentences]\n",
    "    tagged = [nltk.pos_tag(sent) for sent in tokens]\n",
    "    return tagged    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E04CUNb_WU6"
   },
   "source": [
    "Run the cell below to verify your result for the second sentence in the first document.\n",
    "Expected output: \n",
    "`[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R30taRgf_WU6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = docs[0]\n",
    "tagged_sentences = ie_preprocess(first_doc)\n",
    "tagged_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "## Task 3 (20 Marks)\n",
    "Write a function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n",
    "\n",
    "Hint: Use `binary = True` while calling NE chunk function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 approach\n",
    "***\n",
    "I began this task by using the _ne_chunk()_ function from the excercise sheet to create a tree object from a tagged sentence.  \n",
    "  \n",
    "  By iterating through each subtree element of tree.subtrees()  and then similarily the leaves I can extract the named entities from this sentence and append them to my already initialised list \"named_entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5fC0iqJJ_WU_"
   },
   "outputs": [],
   "source": [
    "def find_named_entities(sent):\n",
    "    \n",
    "    named_entities = []\n",
    "    # your code goes here\n",
    "    tree = nltk.ne_chunk(sent, binary=True)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NE':\n",
    "            entity = \"\"\n",
    "            for leaf in subtree.leaves():\n",
    "                entity = entity + leaf[0] + \" \"\n",
    "            named_entities.append(entity.strip())\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td5yJ8cgFScx"
   },
   "source": [
    "Run the cell below to verify your result for the first sentence in the first document.\n",
    "Expected output: `['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FijjdAPWFsp2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = ie_preprocess(docs[0])\n",
    "find_named_entities(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "## Task 4 (5 Marks)\n",
    "\n",
    "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n",
    "\n",
    "Hint: Use `find_named_entities` implemented above for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 approach \n",
    "***\n",
    "The approach to this task was simply combining my previous functions to store all previous entities.\n",
    "\n",
    "  The only 'tricky' part was that I had to be able to get the number of named entities in a document which would mean that the usuual output would not suffice as that would have given me a list in which each element eas another list of the named entities in the sentence.\n",
    "  \n",
    "  My solution was simply to iterate through every output of _find_named_entities()_ and append them to the final list which was initialised outside of the for loops and this returned the flat which made it easy to get the number of entities simply by using the _len()_ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TwFlzQx4_WVF"
   },
   "outputs": [],
   "source": [
    "def find_all_named_entities(doc):\n",
    "    named_entities = []\n",
    "    # your code goes here\n",
    "    tagged = ie_preprocess(doc)\n",
    "    for tagged_sent in tagged:\n",
    "        NE = find_named_entities(tagged_sent)\n",
    "        for entity in NE:\n",
    "            named_entities.append(entity)\n",
    "    return named_entities   # return a flat list and not a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdM0-LZlJy4u"
   },
   "source": [
    "How many named entities did you find in the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_ajmnnOqJ8V6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(len(find_all_named_entities(docs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2AzD9MVNIx2"
   },
   "source": [
    "## Task 5 (5 Marks)\n",
    "\n",
    "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 approach\n",
    "***\n",
    "The challenge in task 5 was similar to that in task 4 in that I needed to return a flat list to again be able to get the number of named entities by just using the _len()_ function alone\n",
    "\n",
    "I also had to reuse my _find_all_named_entities()_ function while iterating through docs (all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YULMcK1-NSR9"
   },
   "outputs": [],
   "source": [
    "all_named_entities = []\n",
    "# your code goes here\n",
    "for doc in docs:\n",
    "    entities = find_all_named_entities(doc)\n",
    "    for entity in entities:\n",
    "        all_named_entities.append(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaM9Cs9zNGM2"
   },
   "source": [
    "How many named entities did you find across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jCNIrC_SNpHQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "len(all_named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "## Task 6 (40 Marks)\n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the `re.compile()` function to create the extraction patterns.\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 approach\n",
    "***\n",
    "From this point onwards I have decided to split each of the final functions into their own cell as to give a good sense of reasoning for my approach and code.\n",
    "\n",
    "  The main method I used which seemed to be repeated for each function was that I would try to find some pattern around the desired entity including it's own format and overall common position in the text to locate the desired information from each text using the same function.\n",
    "  \n",
    "  The most difficult part of this task in my opinion was to find a pattern that would not be seen as over fitting for one certain text (avoid for example 10 patterns surrounded by or statemnets) I wanted to used the least amount of patterns for each function\n",
    "  \n",
    "  In each of the following functions I will explain my patterns(s) , why I chose them and a test iterating over each document in docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### name_of_the_player()\n",
    "***\n",
    "For the name of the player, I immediately noticed that it was usually at the start of the text this was very helpful as I knew if my pattern would pull back multiple matches that the name would most certainly always be the first element in the list of matches.\n",
    "\n",
    "I knew that a name would have a genera,l pattern [A-Z][a-z]+ i.e. a capital letter followed by one or more lowercase letters but the next issue was that I wanted the full name and this would not be a simple case of repeating my current pattern as people had names (a) of different lengths and (b) that had small words e.g. \"de\" within them. \n",
    "\n",
    "I needed to be able to take a flexible amount of characters after the first name but then the issue became when do I ask the pattern to stop matching? From further inspection I could see that the the name always preceeded some information in parenthesis which indicated pronunciations etc. I knew I could ust this open parenthesis as a symbol to stop at.\n",
    "\n",
    "After doing this the only issue left was that some players had accolades after their names for example David Beckham, OBE. This was the only instance that my function would not pull back just the players full name. This was a simple fix in that I just had to alter my pattern to say that it mut end in a comma or an open parenthesis.\n",
    "\n",
    "My pattern was now '[A-Z][a-z]+.*?[,\\(]' which in english was a capital letter followed by one or more lowercase letters followed then by anu character any amount of times until it reaches a ',' or '('. This obviously pulled back a number of matched as there were many proper nouns in the text and lots of punctuation although this brings me back to my initial point that the name would be the first match and so I would always take the first element from the listy returned from the _findall_ function, it is worth noting thaty I also sliced the final value as to remove the trailing ',' or '('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo dos Santos Aveiro\n",
      "Lionel Andrés \"Leo\" Messi \n",
      "Neymar da Silva Santos Júnior \n",
      "Ronaldo de Assis Moreira \n",
      "Wayne Mark Rooney \n",
      "Zlatan Ibrahimović \n",
      "David Robert Joseph Beckham\n",
      "Mesut Özil \n",
      "Gareth Frank Bale \n",
      "Andrés Iniesta Luján \n"
     ]
    }
   ],
   "source": [
    "def name_of_the_player(doc):\n",
    "    name_pattern = re.compile(r'[A-Z][a-z]+.*?[,\\(]')\n",
    "    name = name_pattern.findall(doc)\n",
    "    name = name[0][:-1]\n",
    "    \n",
    "    return name\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    print(name_of_the_player(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date_of_birth()\n",
    "***\n",
    "This function was much more simple to write in that a persons DOB would always have a very unique format.\n",
    "\n",
    "The  pattern I chose was 'born\\d+ [A-Z]\\w+ ?\\d{4}' which in english is at least one digit followed by a word which starts with a capital letter and finally a 4 digit number all seperated by a space.\n",
    "\n",
    "As there were cases of  abother specific date being mentioned in the text I could see there were two possible approaches:\n",
    "\n",
    "    -The first was that I could select the first element as the DOB was always mentioned early in the text.\n",
    "    \n",
    "    -In my opinion a more robust approach would be to also search for the word 'born' which always preceeds the DOB and this is what I did\n",
    "    \n",
    "My final pattern was 'born \\d+ [A-Z]\\w+ ?\\d{4}' , again noting that I sliced the final value to isolate just the DOB. It still appears that I am selecting the first element however this is just a consequence of using pattern.findall() as it always returns a list , the list in this case is of length one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 February 1985\n",
      "24 June 1987\n",
      "5 February 1992\n",
      "21 March 1980\n",
      "24 October 1985\n",
      "3 October 1981\n",
      "2 May 1975\n",
      "15 October 1988\n",
      "16 July 1989\n",
      "11 May 1984\n"
     ]
    }
   ],
   "source": [
    "def date_of_birth(doc):\n",
    "    dob_pattern = re.compile(r'born \\d+ [A-Z]\\w+ ?\\d{4}')\n",
    "    dob = dob_pattern.findall(doc)\n",
    "    dob = dob[0][5:]\n",
    "    \n",
    "    return dob\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    print(date_of_birth(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### country_of_origin()\n",
    "***\n",
    "Extracting the country of origin was once again a fairly straight forward task as I could leverage the patern from the first function (name of player) as a country or nationality would have a capital letter.\n",
    "\n",
    "From investigations I could see that each oof the players had international duties. In this corpus it is always said that the play(ed) for the (country of origin) national team and so I was able to use the presence of the word 'national' to locate their nationality.\n",
    "\n",
    "The pattern I used was '[A-Z][a-z]* national' which means that I am seaching for a word that begins with a capital letter and preceeds the word national.\n",
    "\n",
    "Once again I sliced the word in order to rturn just the country of origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portugal\n",
      "Argentina\n",
      "Brazil\n",
      "Brazil\n",
      "England\n",
      "Sweden\n",
      "England\n",
      "German\n",
      "Wales\n",
      "Spain\n"
     ]
    }
   ],
   "source": [
    "def country_of_origin(doc):\n",
    "    country_pattern = re.compile(r'[A-Z][a-z]* national')\n",
    "    country = country_pattern.findall(doc)\n",
    "    country = country[0][:-9]\n",
    "    \n",
    "    return country\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    print(country_of_origin(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### team_of_the_player()\n",
    "***\n",
    "In my opinion this was the most dificult function as there was no one clear patter for the teams, for this reason I included a number of patterns.\n",
    "\n",
    "The majority of the time the word 'club' preceeds the football club and so this was my starting point for the assignment. I needed to used this twice as there are instances in which the club name consists of one and two words and if I was to try include them both in the same pattern one instance would end up sufferng due to the greedy operator for example.\n",
    "\n",
    "The next step were the club names that weren't preceeded with the word 'club' . However I did notice that in these remaining texts they were all preceeded by the word 'for' and so I could use this to locate them.\n",
    "\n",
    "The final pattern for this function is 'club ?[A-Z][a-z]* a|club ?[A-Z]*[a-z]* [A-Z][a-z]* |for ?[A-Z]+[a-z]* [A-Z][a-z]+.{1}' the addition of one character at the end is to help when slicing at the end so that the other club names are not sliced incorrectly. The strip() is also used to help increase readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Madrid\n",
      "FC Barcelona\n",
      "FC Barcelona\n",
      "FC Barcelona\n",
      "Manchester United\n",
      "Manchester United\n",
      "Manchester United\n",
      "Arsenal\n",
      "Real Madrid\n",
      "FC Barcelona\n"
     ]
    }
   ],
   "source": [
    "def team_of_the_player(doc):\n",
    "    team_pattern = re.compile(r'club ?[A-Z][a-z]* a|club ?[A-Z]*[a-z]* [A-Z][a-z]* |for ?[A-Z]+[a-z]* [A-Z][a-z]+.{1}')\n",
    "    team = team_pattern.findall(doc)\n",
    "    team = team[0][4:-1].strip()\n",
    "    \n",
    "    return team\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    print(team_of_the_player(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### position_of_player()\n",
    "***\n",
    "This was also one of the more difficult functions in that I needed to use 3 patterns.\n",
    "\n",
    "When a players position is being mentioned it is always preceeded by 'is a', 'as a' or in the case where the position starts with a vowel 'as an'\n",
    "\n",
    "The final patter I chose was 'as a [a-z]+.{1}[a-z]* |is a [a-z]+|as an [a-z]+ [a-z]+.* ?' and it is looking for lower case words which are preceeded by the earlier mentioned phrases.\n",
    "\n",
    "To isolate the position I also included a for loop to slice word differently depending on which pattern they were located with, this is the case because some of the postions are being located with the same pattern as some of the multiple word positions such as 'central midfielder' and so it is pulling back the word 'for' afterwards. My solutionwas this was the for loop on the condiotion if the word ends in 'for' an improvemnet I would like to implement would be to generalise this solution somehow eithout afftecting the mutli word positions or to find a better pattern which does this for me. It is worth noting however that without this loop we will still receive the position jsut with the word 'for' after it as this is just to isolate the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "forward\n",
      "forward\n",
      "attacking midfielder\n",
      "forward,\n",
      "striker\n",
      "right winger\n",
      "attacking midfielder\n",
      "winger\n",
      "central midfielder\n"
     ]
    }
   ],
   "source": [
    "def position_of_the_player(doc):\n",
    "    pos_pattern = re.compile(r'as a [a-z]+.{1}[a-z]*|is a [a-z]+|as an [a-z]+ [a-z]+.*?')\n",
    "    pos = pos_pattern.findall(doc)\n",
    "    pos = pos[0][5:].strip()\n",
    "    if pos.endswith('for'):\n",
    "        pos = pos[:-4]\n",
    "    else:\n",
    "        pass\n",
    "    return pos\n",
    "for i in range(len(docs)):\n",
    "    print(position_of_the_player(docs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have used an f sting to iterate through the docements and retuen each of the entities in a sentence that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo dos Santos Aveiro from Portugal, born 5 February 1985\n",
      " plays/played for Real Madrid in the forward position\n",
      "\n",
      "\n",
      "Lionel Andrés \"Leo\" Messi  from Argentina, born 24 June 1987\n",
      " plays/played for FC Barcelona in the forward position\n",
      "\n",
      "\n",
      "Neymar da Silva Santos Júnior  from Brazil, born 5 February 1992\n",
      " plays/played for FC Barcelona in the forward position\n",
      "\n",
      "\n",
      "Ronaldo de Assis Moreira  from Brazil, born 21 March 1980\n",
      " plays/played for FC Barcelona in the attacking midfielder position\n",
      "\n",
      "\n",
      "Wayne Mark Rooney  from England, born 24 October 1985\n",
      " plays/played for Manchester United in the forward, position\n",
      "\n",
      "\n",
      "Zlatan Ibrahimović  from Sweden, born 3 October 1981\n",
      " plays/played for Manchester United in the striker position\n",
      "\n",
      "\n",
      "David Robert Joseph Beckham from England, born 2 May 1975\n",
      " plays/played for Manchester United in the right winger position\n",
      "\n",
      "\n",
      "Mesut Özil  from German, born 15 October 1988\n",
      " plays/played for Arsenal in the attacking midfielder position\n",
      "\n",
      "\n",
      "Gareth Frank Bale  from Wales, born 16 July 1989\n",
      " plays/played for Real Madrid in the winger position\n",
      "\n",
      "\n",
      "Andrés Iniesta Luján  from Spain, born 11 May 1984\n",
      " plays/played for FC Barcelona in the central midfielder position\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(f'{name_of_the_player(doc)} from {country_of_origin(doc)}, born {date_of_birth(doc)}\\n plays/played for {team_of_the_player(doc)} in the {position_of_the_player(doc)} position\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-CNrMM5_WVO"
   },
   "source": [
    "Execute the cell below to verify the `date_of_birth` function for the third player. Expected output `5 February 1992`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "jpeKE1u9_WVP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 February 1992'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_of_birth(docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3VtWxBr_WVZ"
   },
   "source": [
    "## Task 6 (10 Marks)\n",
    "Identify one other relation (besides team and player) and write a function to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "TR0GZrUB_WVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 200 ', ' 130 ']\n",
      "[' 500 ']\n",
      "[]\n",
      "[]\n",
      "[' 246 ', ' 194 ', ' 179 ', ' 118 ']\n",
      "[' 156 ', ' 180 ', ' 100 ']\n",
      "[' 100 ', ' 100 ', ' 115 ']\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def debut_date(doc):\n",
    "    debut_pattern = re.compile(r' \\d{3} ')\n",
    "    debut_date = debut_pattern.findall(doc)\n",
    "\n",
    "    return debut_date\n",
    "for i in range(len(docs)):\n",
    "    print(debut_date(docs[i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": [
    {
     "file_id": "1EXXdimBbQY8nnqIs5hBXdG68r2hsk7HS",
     "timestamp": 1604940588321
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
